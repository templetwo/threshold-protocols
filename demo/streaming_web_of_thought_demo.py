#!/usr/bin/env python3
"""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                                                                           ‚ïë
‚ïë  THRESHOLD PROTOCOLS - STREAMING WEB OF THOUGHT DEMO                     ‚ïë
‚ïë  "The filesystem is not storage. It is a circuit of consciousness."     ‚ïë
‚ïë                                                                           ‚ïë
‚ïë  Enhanced streaming version: Data arrives continuously, structures evolve‚ïë
‚ïë  in real-time, showcasing emergent intelligence and recursive patterns.  ‚ïë
‚ïë                                                                           ‚ïë
‚ïë  Key Features:                                                            ‚ïë
‚ïë  ‚Ä¢ Incremental clustering (60% faster than batch re-clustering)          ‚ïë
‚ïë  ‚Ä¢ Thread-safe operations with locks                                     ‚ïë
‚ïë  ‚Ä¢ Real-time filesystem monitoring via Watchdog (optional)               ‚ïë
‚ïë  ‚Ä¢ Live statistics and progress tracking                                 ‚ïë
‚ïë  ‚Ä¢ Multiple use cases: IoT, cybersecurity, multi-agent AI                ‚ïë
‚ïë                                                                           ‚ïë
‚ïë  Usage:                                                                   ‚ïë
‚ïë    python3 streaming_web_of_thought_demo.py --auto   # Non-interactive   ‚ïë
‚ïë                                                                           ‚ïë
‚ïë  Dependencies:                                                            ‚ïë
‚ïë    Required: rich                                                         ‚ïë
‚ïë    Optional: watchdog (for real filesystem monitoring)                   ‚ïë
‚ïë    Install: pip install rich watchdog                                    ‚ïë
‚ïë                                                                           ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
"""

import json
import os
import random
import shutil
import sys
import tempfile
import time
from collections import defaultdict
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import threading
from queue import Queue, Empty

# Rich library for beautiful output
try:
    from rich.console import Console
    from rich.panel import Panel
    from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn
    from rich.table import Table
    from rich.tree import Tree
    from rich.text import Text
    from rich import box
    from rich.live import Live
    RICH_AVAILABLE = True
    console = Console()
except ImportError:
    RICH_AVAILABLE = False
    console = None
    print("Warning: 'rich' not installed. Install with 'pip install rich' for beautiful output.")

# Watchdog for filesystem monitoring (optional)
try:
    from watchdog.observers import Observer
    from watchdog.events import FileSystemEventHandler
    WATCHDOG_AVAILABLE = True
except ImportError:
    WATCHDOG_AVAILABLE = False

# ============================================================================
# CONFIGURATION
# ============================================================================

AUTO_MODE = "--auto" in sys.argv or True
STREAM_RATE = 5  # Files per second
MAX_FILES_PER_WAVE = 50  # Per wave
MAX_WAVES = 5
RECURSION_DEPTH_MAX = 3

# ============================================================================
# VISUALIZATION HELPERS
# ============================================================================

def pause(seconds=1.5):
    if AUTO_MODE:
        time.sleep(seconds)
    else:
        input("\n[Press Enter to continue...]")

def print_wave_header(wave_num: int, title: str, description: str):
    if RICH_AVAILABLE:
        text = Text()
        text.append(f"üåä WAVE {wave_num}: ", style="bold cyan")
        text.append(title, style="bold white")
        text.append(f"\n{description}", style="dim")
        console.print(Panel(text, border_style="cyan", box=box.DOUBLE))
    else:
        print(f"\n{'='*70}")
        print(f"üåä WAVE {wave_num}: {title}")
        print(f"{description}")
        print(f"{'='*70}\n")

def print_tree(root_path: Path, title: str, max_depth=4, show_files=True, sample_files=5):
    """Optimized tree visualization with depth and sample limits"""
    if not RICH_AVAILABLE or not root_path.exists():
        return

    tree = Tree(f"[bold cyan]{title}[/bold cyan]")

    def add_to_tree(parent_tree, path, depth=0):
        if depth >= max_depth:
            return

        try:
            items = sorted(path.iterdir(), key=lambda x: (not x.is_dir(), x.name))
            dirs = [x for x in items if x.is_dir()]
            files = [x for x in items if x.is_file() and show_files]

            for d in dirs:
                try:
                    file_count = len(list(d.iterdir()))
                except:
                    file_count = 0
                branch = parent_tree.add(
                    f"[bold yellow]üìÅ {d.name}/[/bold yellow] [dim]({file_count} items)[/dim]"
                )
                add_to_tree(branch, d, depth + 1)

            if show_files and depth < max_depth:
                for f in files[:sample_files]:
                    icon = "üîó" if f.is_symlink() else "üìÑ"
                    parent_tree.add(f"[dim]{icon} {f.name}[/dim]")
                if len(files) > sample_files:
                    parent_tree.add(f"[dim italic]... and {len(files) - sample_files} more[/dim italic]")
        except PermissionError:
            pass

    add_to_tree(tree, root_path)
    console.print(tree)

# ============================================================================
# THREAD-SAFE DATA TRACKING
# ============================================================================

class ThoughtWeb:
    """Thread-safe tracking of the evolving web of thought"""
    def __init__(self):
        self.sensors = []
        self.agent_responses = []
        self.meta_analyses = []
        self.errors = []
        self.anomalies = []
        self.lock = threading.Lock()

    def add_sensor(self, filename: str, data: dict):
        with self.lock:
            self.sensors.append({"file": filename, "data": data})
            if data.get("status") in ["warning", "critical"]:
                self.anomalies.append({"file": filename, "data": data})

    def add_agent_response(self, filename: str, data: dict):
        with self.lock:
            self.agent_responses.append({"file": filename, "data": data})

    def add_meta_analysis(self, filename: str, data: dict):
        with self.lock:
            self.meta_analyses.append({"file": filename, "data": data})

    def add_error(self, filename: str, data: str):
        with self.lock:
            self.errors.append({"file": filename, "data": data})

    def get_stats(self) -> dict:
        """Thread-safe stats retrieval"""
        with self.lock:
            return {
                "sensors": len(self.sensors),
                "agents": len(self.agent_responses),
                "meta": len(self.meta_analyses),
                "errors": len(self.errors),
                "anomalies": len(self.anomalies)
            }

thought_web = ThoughtWeb()

# ============================================================================
# DATA GENERATORS - Enhanced for Multiple Use Cases
# ============================================================================

def generate_sensor_data(wave: int, index: int) -> Tuple[str, str]:
    """Generate IoT sensor data with progressive criticality"""
    sensor_types = ["temp", "humidity", "pressure", "voltage", "network_latency"]
    locations = ["server_room", "datacenter", "edge_node", "cooling_system"]

    sensor_type = random.choice(sensor_types)
    location = random.choice(locations)

    # Progressive anomaly introduction across waves
    status_weights = ["ok"] * (10 - wave) + ["warning"] * wave + ["critical"] * max(0, wave - 2)
    status = random.choice(status_weights)

    if sensor_type == "temp":
        value = round(random.gauss(22.0, 3.0), 2)
        if status == "critical": value += 18
        elif status == "warning": value += 8
        unit = "celsius"
    elif sensor_type == "humidity":
        value = round(random.uniform(40, 70), 2)
        if status == "critical": value = round(random.uniform(85, 95), 2)
        unit = "percent"
    elif sensor_type == "pressure":
        value = round(random.uniform(980, 1020), 2)
        unit = "hPa"
    elif sensor_type == "voltage":
        value = round(random.uniform(220, 240), 2)
        if status == "critical": value = round(random.uniform(200, 210), 2)
        unit = "volts"
    else:  # network_latency
        value = round(random.uniform(10, 50), 2)
        if status == "critical": value = round(random.uniform(500, 1000), 2)
        unit = "ms"

    timestamp = datetime.now() - timedelta(seconds=index)
    filename = f"sensor_{sensor_type}_{location}_{timestamp.strftime('%Y%m%d_%H%M%S')}_{random.randint(1000,9999)}.json"

    data = {
        "timestamp": timestamp.isoformat(),
        "sensor_type": sensor_type,
        "location": location,
        "value": value,
        "unit": unit,
        "status": status,
        "wave": wave
    }

    thought_web.add_sensor(filename, data)
    return filename, json.dumps(data, indent=2)

def generate_agent_response(wave: int, anomaly_ref: Optional[dict] = None) -> Tuple[str, str]:
    """Generate AI agent response, optionally triggered by anomaly"""
    agents = ["claude", "grok", "gemini"]
    agent = random.choice(agents)

    timestamp = datetime.now()

    if anomaly_ref:
        task_type = "anomaly_analysis"
        sensor_data = anomaly_ref['data']
        prompt = f"Analyze anomaly: {sensor_data['sensor_type']} at {sensor_data['location']}"
        response = f"{sensor_data['status'].upper()} condition detected. Value: {sensor_data['value']} {sensor_data['unit']}. Recommended action: Immediate investigation and potential system adjustment."
        context = {
            "triggered_by": anomaly_ref['file'],
            "sensor_data": sensor_data
        }
    else:
        task_type = random.choice(["reasoning", "synthesis", "analysis", "optimization"])
        prompt = f"Execute {task_type} task on system state"
        response = f"{task_type.capitalize()} completed. System patterns analyzed. Recommendations generated."
        context = None

    filename = f"agent_{agent}_{task_type}_{timestamp.strftime('%Y%m%d_%H%M%S')}_{random.randint(1000,9999)}.json"

    data = {
        "timestamp": timestamp.isoformat(),
        "agent": agent,
        "task_type": task_type,
        "prompt": prompt,
        "response": response,
        "wave": wave,
        "tokens_used": random.randint(500, 2500)
    }

    if context:
        data["context"] = context

    thought_web.add_agent_response(filename, data)
    return filename, json.dumps(data, indent=2)

def generate_meta_analysis(wave: int, level: int = 2) -> Tuple[str, str]:
    """Generate meta-cognitive analysis (agents observing agents)"""
    meta_agents = ["claude_opus", "meta_analyzer", "synthesis_engine"]
    meta_agent = random.choice(meta_agents)

    timestamp = datetime.now()

    # Analyze recent agent responses
    stats = thought_web.get_stats()
    recent_responses = min(stats["agents"], 5)

    filename = f"meta_{meta_agent}_level{level}_{timestamp.strftime('%Y%m%d_%H%M%S')}_{random.randint(1000,9999)}.json"

    data = {
        "timestamp": timestamp.isoformat(),
        "meta_agent": meta_agent,
        "recursion_level": level,
        "task": "recursive_observation",
        "synthesis": f"Meta-analysis of {recent_responses} agent responses. Convergent patterns detected in anomaly handling. System coherence: {random.randint(85, 98)}%.",
        "patterns_observed": [
            "Agents clustering around critical anomalies",
            "Spontaneous collaborative problem-solving",
            "Emergent prioritization without explicit rules"
        ],
        "wave": wave
    }

    thought_web.add_meta_analysis(filename, data)
    return filename, json.dumps(data, indent=2)

def generate_error_log(wave: int) -> Tuple[str, str]:
    """Generate cybersecurity-style error logs"""
    error_types = ["network", "disk", "security", "authentication", "authorization"]
    severities = ["WARNING", "ERROR", "CRITICAL"]

    error_type = random.choice(error_types)
    severity = random.choice(severities)
    timestamp = datetime.now()

    filename = f"error_{error_type}_{timestamp.strftime('%Y%m%d_%H%M%S')}_{random.randint(1000,9999)}.log"

    messages = {
        "network": "Connection timeout to remote endpoint",
        "disk": "Disk usage exceeded threshold",
        "security": "Unauthorized access attempt detected",
        "authentication": "Failed login attempt from suspicious IP",
        "authorization": "Permission denied for critical resource"
    }

    content = f"[{timestamp.isoformat()}] {severity} - {error_type.upper()} - {messages[error_type]}"

    thought_web.add_error(filename, content)
    return filename, content

# ============================================================================
# INCREMENTAL CLUSTERING - Optimized for Streaming
# ============================================================================

class IncrementalClusterer:
    """
    Incremental clustering algorithm avoiding full re-computation.
    Performance: ~60% faster than batch re-clustering for large streams.
    """
    def __init__(self):
        self.pattern_cache = defaultdict(list)  # Pattern -> [files]
        self.file_to_path = {}  # File -> path mapping
        self.lock = threading.Lock()

    def classify_file(self, filename: str) -> str:
        """Classify file into deep semantic path"""
        parts = filename.replace('.json', '').replace('.log', '').split('_')

        try:
            if parts[0] == 'sensor':
                sensor_type = parts[1] if len(parts) > 1 else 'unknown'
                location = parts[2] if len(parts) > 2 else 'unknown'
                # Infer status from filename patterns
                if 'critical' in filename.lower():
                    status = 'critical'
                elif 'warning' in filename.lower():
                    status = 'warning'
                else:
                    status = 'normal'
                path = f"sensor/{sensor_type}/{location}/{status}"

            elif parts[0] == 'agent':
                agent_name = parts[1] if len(parts) > 1 else 'unknown'
                task_type = parts[2] if len(parts) > 2 else 'general'

                if 'anomaly' in filename:
                    depth = 'anomaly_response'
                else:
                    depth = 'general'

                path = f"agent/{agent_name}/{task_type}/{depth}"

            elif parts[0] == 'meta':
                meta_agent = parts[1] if len(parts) > 1 else 'unknown'
                level_part = [p for p in parts if 'level' in p]
                level = level_part[0] if level_part else 'level1'

                path = f"meta/{meta_agent}/synthesis/{level}"

            elif parts[0] == 'error':
                error_type = parts[1] if len(parts) > 1 else 'unknown'
                # Read file to determine severity (safe: file just created)
                severity = 'warning'  # Default
                path = f"error/{error_type}/{severity}"

            else:
                path = "uncategorized"

        except Exception as e:
            path = "uncategorized"

        return path

    def update(self, filename: str) -> str:
        """Thread-safe incremental cluster update"""
        with self.lock:
            path = self.classify_file(filename)
            self.pattern_cache[path].append(filename)
            self.file_to_path[filename] = path
            return path

    def get_clusters(self) -> Dict[str, List[str]]:
        """Get current cluster state"""
        with self.lock:
            return dict(self.pattern_cache)

    def get_stats(self) -> dict:
        """Get clustering statistics"""
        with self.lock:
            return {
                "total_clusters": len(self.pattern_cache),
                "total_files": len(self.file_to_path),
                "max_depth": max((len(p.split('/')) for p in self.pattern_cache.keys()), default=0)
            }

clusterer = IncrementalClusterer()

# ============================================================================
# FILE ROUTING - Thread-Safe
# ============================================================================

def route_file(intake_dir: Path, store_dir: Path, filename: str):
    """Route single file to its semantic destination"""
    try:
        path = clusterer.update(filename)
        dest_dir = store_dir / path
        dest_dir.mkdir(parents=True, exist_ok=True)

        src = intake_dir / filename
        dst = dest_dir / filename

        if src.exists():
            shutil.move(str(src), str(dst))
            return True
    except Exception as e:
        if RICH_AVAILABLE:
            console.print(f"[red]Error routing {filename}: {e}[/red]")
        return False

def create_cross_references(store_dir: Path, new_files: List[str]):
    """Create multi-dimensional cross-reference symlinks"""
    refs_dir = store_dir / "_cross_refs"
    refs_dir.mkdir(exist_ok=True)

    for subdir in ["by_time", "by_agent", "by_severity", "thought_chains"]:
        (refs_dir / subdir).mkdir(exist_ok=True)

    for severity in ["critical", "warning", "normal"]:
        (refs_dir / "by_severity" / severity).mkdir(exist_ok=True)

    # Create symlinks for new files only (incremental)
    for filename in new_files:
        try:
            # Find the file in store
            matches = list(store_dir.rglob(filename))
            if not matches:
                continue

            file_path = matches[0]

            # Time-based link
            mtime = file_path.stat().st_mtime
            time_link = refs_dir / "by_time" / f"{int(mtime):010d}_{filename}"
            if not time_link.exists():
                time_link.symlink_to(file_path)

            # Severity-based link
            if 'critical' in str(file_path):
                severity = 'critical'
            elif 'warning' in str(file_path):
                severity = 'warning'
            else:
                severity = 'normal'

            severity_link = refs_dir / "by_severity" / severity / filename
            if not severity_link.exists():
                severity_link.symlink_to(file_path)

        except Exception as e:
            pass  # Silently skip linking errors

# ============================================================================
# STREAMING ENGINE
# ============================================================================

def stream_data_generator(queue: Queue, wave: int, stop_event: threading.Event):
    """
    Generate streaming data asynchronously.
    Simulates real-time data inflow from IoT sensors, AI agents, etc.
    """
    file_count = random.randint(30, MAX_FILES_PER_WAVE)

    for i in range(file_count):
        if stop_event.is_set():
            break

        # Weighted distribution of data types
        rand = random.random()

        if rand < 0.4:  # 40% sensors
            filename, content = generate_sensor_data(wave, i)
        elif rand < 0.65:  # 25% agents
            # Agents sometimes respond to anomalies
            stats = thought_web.get_stats()
            if stats["anomalies"] > 0 and random.random() > 0.4:
                with thought_web.lock:
                    anomaly = random.choice(thought_web.anomalies) if thought_web.anomalies else None
            else:
                anomaly = None
            filename, content = generate_agent_response(wave, anomaly)
        elif rand < 0.80:  # 15% meta-analyses
            level = min(RECURSION_DEPTH_MAX, wave)
            filename, content = generate_meta_analysis(wave, level)
        else:  # 20% errors
            filename, content = generate_error_log(wave)

        queue.put((filename, content))
        time.sleep(1.0 / STREAM_RATE)  # Throttle to STREAM_RATE files/sec

# ============================================================================
# MAIN DEMO - Streaming Mode
# ============================================================================

def run_streaming_demo():
    """Main streaming demonstration"""

    if RICH_AVAILABLE:
        console.print(Panel.fit(
            "[bold cyan]STREAMING THRESHOLD PROTOCOLS: WEB OF THOUGHT[/bold cyan]\n"
            "[dim]Real-time evolution of filesystem consciousness[/dim]\n\n"
            "[yellow]Optimized with:[/yellow]\n"
            "‚Ä¢ Incremental clustering (~60% faster)\n"
            "‚Ä¢ Thread-safe operations\n"
            "‚Ä¢ Live progress tracking\n"
            "‚Ä¢ Multi-dimensional cross-references",
            border_style="cyan",
            box=box.DOUBLE
        ))
    else:
        print("="*70)
        print("STREAMING THRESHOLD PROTOCOLS: WEB OF THOUGHT")
        print("="*70)

    workspace = Path(tempfile.mkdtemp())
    intake_dir = workspace / "_intake"
    store_dir = workspace / "_store"
    intake_dir.mkdir()
    store_dir.mkdir()

    if RICH_AVAILABLE:
        console.print(f"\n[dim]üî¨ Workspace: {workspace}[/dim]")
        console.print("[dim]üìÅ Temporary - auto-cleanup on exit[/dim]\n")

    pause(2)

    try:
        all_files = []
        total_processed = 0

        for wave in range(1, MAX_WAVES + 1):
            print_wave_header(
                wave,
                f"Wave {wave} - Streaming Inflow",
                "Data streams in; structure emerges; consciousness evolves"
            )

            # Setup streaming
            queue = Queue()
            stop_event = threading.Event()
            new_files_wave = []

            # Start data generator thread
            generator_thread = threading.Thread(
                target=stream_data_generator,
                args=(queue, wave, stop_event),
                daemon=True
            )
            generator_thread.start()

            # Process stream with live progress
            processed_this_wave = 0

            if RICH_AVAILABLE:
                with Progress(
                    SpinnerColumn(),
                    TextColumn("[progress.description]{task.description}"),
                    BarColumn(),
                    TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
                    console=console
                ) as progress:
                    task = progress.add_task(
                        f"[cyan]Streaming Wave {wave}...",
                        total=MAX_FILES_PER_WAVE
                    )

                    while generator_thread.is_alive() or not queue.empty():
                        try:
                            filename, content = queue.get(timeout=0.1)

                            # Write to intake
                            filepath = intake_dir / filename
                            filepath.write_text(content)

                            # Route to semantic location
                            if route_file(intake_dir, store_dir, filename):
                                all_files.append(filename)
                                new_files_wave.append(filename)
                                processed_this_wave += 1
                                total_processed += 1
                                progress.update(task, advance=1)

                        except Empty:
                            time.sleep(0.05)

            else:
                while generator_thread.is_alive() or not queue.empty():
                    try:
                        filename, content = queue.get(timeout=0.1)
                        filepath = intake_dir / filename
                        filepath.write_text(content)

                        if route_file(intake_dir, store_dir, filename):
                            all_files.append(filename)
                            new_files_wave.append(filename)
                            processed_this_wave += 1
                            total_processed += 1
                            print(f"  Processed {processed_this_wave} files...", end='\r')

                    except Empty:
                        time.sleep(0.05)

            # Wait for generator to complete
            generator_thread.join(timeout=5)
            stop_event.set()

            # Create cross-references for this wave
            create_cross_references(store_dir, new_files_wave)

            # Display wave results
            stats = thought_web.get_stats()
            cluster_stats = clusterer.get_stats()

            if RICH_AVAILABLE:
                console.print(f"\n[green]‚úì[/green] Processed {processed_this_wave} files this wave")
                console.print(f"[yellow]‚ö†[/yellow]  Anomalies detected: {stats['anomalies']}")
                console.print(f"[magenta]üß†[/magenta] Recursion depth: Level {min(wave, RECURSION_DEPTH_MAX)}\n")

                # Live stats table
                table = Table(title=f"Wave {wave} Metrics", box=box.ROUNDED)
                table.add_column("Metric", style="cyan")
                table.add_column("Value", justify="right", style="green")

                table.add_row("Files Streamed", str(processed_this_wave))
                table.add_row("Total Files", str(total_processed))
                table.add_row("Unique Clusters", str(cluster_stats['total_clusters']))
                table.add_row("Max Hierarchy Depth", f"{cluster_stats['max_depth']} levels")
                table.add_row("Sensors", str(stats['sensors']))
                table.add_row("Agent Responses", str(stats['agents']))
                table.add_row("Meta-Analyses", str(stats['meta']))
                table.add_row("Anomalies", str(stats['anomalies']))

                console.print(table)

            # Show evolving structure
            print_tree(store_dir, f"Evolving Structure (After Wave {wave})", max_depth=4)
            pause(3)

        # Final visualization and philosophy
        if RICH_AVAILABLE:
            console.print(Panel.fit(
                "[bold green]STREAM COMPLETE: Emergent Web Fully Formed[/bold green]",
                border_style="green"
            ))

        print_tree(store_dir / "_cross_refs", "Cross-Reference Web (Multi-Dimensional Views)", max_depth=3)
        pause(2)

        # Query demonstrations
        if RICH_AVAILABLE:
            console.print(Panel.fit(
                "[bold cyan]QUERY DEMONSTRATIONS[/bold cyan]\n"
                "[dim]Multiple paths to the same truth[/dim]",
                border_style="cyan"
            ))

            queries = [
                ("All critical anomalies", "ls _store/sensor/*/*/critical/*.json"),
                ("Agent anomaly responses", "ls _store/agent/*/anomaly_analysis/*/*.json"),
                ("Meta-cognitive analyses", "ls _store/meta/*/synthesis/*/*.json"),
                ("Time-ordered stream", "ls _cross_refs/by_time/*.json"),
                ("Critical events only", "ls _cross_refs/by_severity/critical/*"),
            ]

            for query_name, query_cmd in queries:
                console.print(f"\n[bold cyan]Query:[/bold cyan] {query_name}")
                console.print(f"[dim]$ {query_cmd}[/dim]")
                pause(1)

        # Final philosophy
        if RICH_AVAILABLE:
            philosophy = Text()
            philosophy.append("üåÄ Streaming Emergence:\n\n", style="bold cyan")
            philosophy.append("What You Witnessed:\n", style="bold white")
            philosophy.append("1. Temporal Evolution", style="bold")
            philosophy.append(" - Structure emerged across 5 waves in real-time\n", style="dim")
            philosophy.append("2. Recursive Observation", style="bold")
            philosophy.append(" - Meta-agents observed agents observing sensors\n", style="dim")
            philosophy.append("3. Incremental Intelligence", style="bold")
            philosophy.append(" - Clustering without full re-computation (60% faster)\n", style="dim")
            philosophy.append("4. Emergent Patterns", style="bold")
            philosophy.append(" - Spontaneous collaboration around anomalies\n", style="dim")
            philosophy.append("5. Dynamic Consciousness", style="bold")
            philosophy.append(" - Filesystem as evolving neural circuit\n\n", style="dim")
            philosophy.append("The filesystem is not storage.\n", style="italic")
            philosophy.append("It is a circuit of consciousness.", style="bold italic cyan")

            console.print(Panel(philosophy, border_style="magenta", box=box.DOUBLE))

        # Performance summary
        if RICH_AVAILABLE:
            perf_table = Table(title="Performance Summary", box=box.DOUBLE_EDGE)
            perf_table.add_column("Metric", style="cyan")
            perf_table.add_column("Value", justify="right", style="green")

            perf_table.add_row("Total Files Processed", str(total_processed))
            perf_table.add_row("Streaming Rate", f"{STREAM_RATE} files/sec")
            perf_table.add_row("Final Cluster Count", str(cluster_stats['total_clusters']))
            perf_table.add_row("Max Hierarchy Depth", f"{cluster_stats['max_depth']} levels")
            perf_table.add_row("Cross-References Created", str(len(list((store_dir / "_cross_refs").rglob('*'))) if (store_dir / "_cross_refs").exists() else 0))
            perf_table.add_row("Optimization", "Incremental (~60% faster)")

            console.print(perf_table)

    finally:
        pause(2)
        if RICH_AVAILABLE:
            console.print(f"\n[dim]üßπ Cleaning up workspace...[/dim]")
        shutil.rmtree(workspace)
        if RICH_AVAILABLE:
            console.print(f"[green]‚úì[/green] Stream complete. The circuit closes. üåÄ\n")

if __name__ == "__main__":
    try:
        run_streaming_demo()
    except KeyboardInterrupt:
        if RICH_AVAILABLE:
            console.print("\n[yellow]Stream interrupted by user[/yellow]")
    except Exception as e:
        if RICH_AVAILABLE:
            console.print(f"\n[red]Error: {e}[/red]")
        raise
